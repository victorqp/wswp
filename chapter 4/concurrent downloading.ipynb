{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as urlreq\n",
    "import urllib.error as urlerr\n",
    "import urllib.parse as urlparse\n",
    "import urllib.robotparser as urlrp\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from common.utils import *\n",
    "from common.db_cache import *\n",
    "from common.sequential_crawler import *\n",
    "from common.threaded_crawler import *\n",
    "from common.process_crawler import *\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO, TextIOWrapper\n",
    "\n",
    "class AlexaCallback():\n",
    "    def __init__(self, max_urls=50):\n",
    "        self.max_urls = max_urls;\n",
    "        self.seed_url = 'http://s3.amazonaws.com/alexa-static/top-1m.csv.zip'\n",
    "        \n",
    "    def __call__(self, url, html):\n",
    "        if url == self.seed_url:\n",
    "            urls = []\n",
    "            with ZipFile(BytesIO(html)) as zf:\n",
    "                csv_filename = zf.namelist()[0]\n",
    "                for _, website in csv.reader(TextIOWrapper(zf.open(csv_filename))):\n",
    "                    urls.append('http://' + website)\n",
    "                    if len(urls) == self.max_urls:\n",
    "                        break\n",
    "        \n",
    "            return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:  http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\n",
      "Downloading:  http://bing.com\n",
      "Downloading:  http://mail.ru\n",
      "Downloading:  http://google.com.hk\n",
      "Downloading:  http://office.com\n",
      "Downloading:  http://microsoft.com\n",
      "Downloading:  http://ebay.com\n",
      "Downloading:  http://ok.ru\n",
      "Downloading:  http://hao123.com\n",
      "Downloading:  http://alipay.com\n",
      "Downloading:  http://google.ca\n",
      "Downloading:  http://xvideos.com\n",
      "Downloading:  http://pages.tmall.com\n",
      "Downloading:  http://t.co\n",
      "Downloading:  http://google.com.mx\n",
      "Downloading:  http://yahoo.co.jp\n",
      "Downloading:  http://pornhub.com\n",
      "Downloading:  http://google.es\n",
      "Downloading:  http://twitch.tv\n",
      "Downloading:  http://google.it\n",
      "Downloading:  http://netflix.com\n",
      "Downloading:  http://linkedin.com\n",
      "Downloading:  http://google.ru\n",
      "Downloading:  http://google.fr\n",
      "Downloading:  http://yandex.ru\n",
      "Downloading:  http://google.com.br\n",
      "Downloading:  http://google.co.uk\n",
      "Downloading:  http://google.de\n",
      "Downloading:  http://weibo.com\n",
      "Downloading:  http://jd.com\n",
      "Downloading:  http://sina.com.cn\n",
      "Downloading:  http://360.cn\n",
      "Downloading:  http://vk.com\n",
      "Downloading:  http://google.co.jp\n",
      "Downloading:  http://live.com\n",
      "Downloading:  http://instagram.com\n",
      "Downloading:  http://amazon.com\n",
      "Downloading:  http://sohu.com\n",
      "Downloading:  http://twitter.com\n",
      "Downloading:  http://taobao.com\n",
      "Downloading:  http://tmall.com\n",
      "Downloading:  http://qq.com\n",
      "Downloading:  http://google.co.in\n",
      "Downloading:  http://reddit.com\n",
      "Downloading:  http://yahoo.com\n",
      "Downloading:  http://wikipedia.org\n",
      "Downloading:  http://baidu.com\n",
      "Downloading:  http://facebook.com\n",
      "Downloading:  http://youtube.com\n",
      "Downloading:  http://google.com\n",
      "sequential download: 115.78 seconds\n"
     ]
    }
   ],
   "source": [
    "scrape_callback = AlexaCallback()\n",
    "\n",
    "cache = DBCache()\n",
    "start = time.time()\n",
    "all_links = link_crawler(scrape_callback.seed_url, user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',\n",
    "                         scrape_callback=scrape_callback, cache=cache, ignore_robots=True)\n",
    "end = time.time()\n",
    "print(\"sequential download: %.2f seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:  http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\n",
      "Downloading:  http://bing.com\n",
      "Downloading:  http://mail.ru\n",
      "Downloading:  http://google.com.hkDownloading: \n",
      " Downloading: http://microsoft.com \n",
      "Downloading: http://office.comDownloading:  Downloading: Downloading: \n",
      " http://ok.ru  http://google.cahttp://alipay.com\n",
      "http://xvideos.com\n",
      "\n",
      "\n",
      "Downloading:  http://ebay.com\n",
      "Downloading:  http://hao123.com\n",
      "Downloading:  http://pages.tmall.com\n",
      "Downloading:  http://t.co\n",
      "Downloading:  http://google.com.mx\n",
      "Downloading:  http://yahoo.co.jp\n",
      "Downloading:  http://pornhub.com\n",
      "Downloading:  http://google.es\n",
      "Downloading:  http://twitch.tv\n",
      "Downloading:  http://google.it\n",
      "Downloading:  http://netflix.com\n",
      "Downloading:  http://linkedin.com\n",
      "Downloading:  http://google.ru\n",
      "Downloading:  http://google.fr\n",
      "Downloading:  http://yandex.ru\n",
      "Downloading:  http://google.com.br\n",
      "Downloading:  http://google.co.uk\n",
      "Downloading:  http://google.de\n",
      "Downloading:  http://login.tmall.com\n",
      "Downloading:  http://weibo.com\n",
      "Downloading:  http://jd.com\n",
      "Downloading:  http://sina.com.cn\n",
      "Downloading:  http://360.cn\n",
      "Downloading:  Downloading: http://vk.com \n",
      "http://google.co.jp\n",
      "Downloading:  http://live.com\n",
      "Downloading:  http://instagram.com\n",
      "Downloading:  http://amazon.com\n",
      "Downloading:  http://sohu.com\n",
      "Downloading:  http://twitter.com\n",
      "Downloading:  http://taobao.com\n",
      "Downloading:  http://tmall.com\n",
      "Downloading:  http://qq.com\n",
      "Downloading:  http://google.co.in\n",
      "Downloading:  http://reddit.com\n",
      "Downloading:  http://yahoo.com\n",
      "Downloading:  http://wikipedia.org\n",
      "Downloading:  http://baidu.com\n",
      "Downloading:  http://facebook.com\n",
      "Downloading:  http://youtube.com\n",
      "Downloading:  http://google.com\n",
      "threaded download: 23.04 seconds\n"
     ]
    }
   ],
   "source": [
    "cache.clear()\n",
    "cache = DBCache()\n",
    "start = time.time()\n",
    "threaded_crawler(scrape_callback.seed_url, scrape_callback=scrape_callback, cache=cache, max_threads=10, timeout=10)\n",
    "end = time.time()\n",
    "print(\"threaded download: %.2f seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocess doesn't work in interactive editor, use the process_test.py instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
